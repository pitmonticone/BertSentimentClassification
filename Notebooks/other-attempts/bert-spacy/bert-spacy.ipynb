{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#####run only once (the first time)\n",
    "#!pip install --upgrade spacy #incompatible with (!pip install spacy-pytorch-transformers)\n",
    "#!pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html  #this is useful only on jupyter\n",
    "#!pip uninstall -y spacy-pytorch-transformers #this fixes #un\n",
    "!pip install --upgrade spacy\n",
    "#!pip install spacy-transformers\n",
    "!pip install thinc_gpu_ops # uncomemnt thia if you want the gpu to run on kaggle\n",
    "#!pip install spacy-pytorch-transformers #it is deprecated https://github.com/explosion/spaCy/issues/4382\n",
    "!python -m spacy download en_trf_bertbaseuncased_lg  # only this on kaggle. Do not try to load a dataset with bert inside it won't work\n",
    "#on kaggle, uncomment only this! \n",
    "#####\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import math   \n",
    "from spacy.util import minibatch , compounding \n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.utils import shuffle\n",
    "from spacy_transformers.util import cyclic_triangular_rate\n",
    "from collections import Counter\n",
    "import re\n",
    "from thinc.neural.optimizers import Adam\n",
    "from thinc.neural import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#enable gpu\n",
    "\n",
    "import thinc_gpu_ops\n",
    "print(thinc_gpu_ops.AVAILABLE) \n",
    "is_using_gpu = spacy.prefer_gpu()\n",
    "if is_using_gpu:\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    print(\"done\")\n",
    "\n",
    "\n",
    "#end of gpu part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting spacy  \n",
    "  Downloading spacy-2.2.4-cp36-cp36m-manylinux1_x86_64.whl (10.6 MB)  \n",
    "     |████████████████████████████████| 10.6 MB 2.8 MB/s   \n",
    "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.6/site-packages (from spacy) (46.1.3.post20200330)  \n",
    "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.6.0)  \n",
    "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (1.0.2)  \n",
    "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (2.22.0)  \n",
    "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (1.18.2)  \n",
    "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.9.6)  \n",
    "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy) (2.0.3)  \n",
    "Collecting thinc==7.4.0  \n",
    "  Downloading thinc-7.4.0-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)  \n",
    "     |████████████████████████████████| 2.2 MB 23.3 MB/s   \n",
    "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (4.42.0)  \n",
    "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy) (1.0.2)  \n",
    "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy) (3.0.2)  \n",
    "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.6/site-packages (from spacy) (1.0.0)  \n",
    "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from spacy) (0.4.1)  \n",
    "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)  \n",
    "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)  \n",
    "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)  \n",
    "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)  \n",
    "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)  \n",
    "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.2.0)  \n",
    "ERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.4 which is incompatible.  \n",
    "Installing collected packages: thinc, spacy  \n",
    "  Attempting uninstall: thinc  \n",
    "    Found existing installation: thinc 7.3.1  \n",
    "    Uninstalling thinc-7.3.1:  \n",
    "      Successfully uninstalled thinc-7.3.1  \n",
    "  Attempting uninstall: spacy  \n",
    "    Found existing installation: spacy 2.2.3  \n",
    "    Uninstalling spacy-2.2.3:  \n",
    "      Successfully uninstalled spacy-2.2.3  \n",
    "Successfully installed spacy-2.2.4 thinc-7.4.0  \n",
    "Collecting thinc_gpu_ops  \n",
    "  Downloading thinc_gpu_ops-0.0.4.tar.gz (483 kB)  \n",
    "     |████████████████████████████████| 483 kB 2.8 MB/s   \n",
    "Requirement already satisfied: numpy>=1.7.0 in /opt/conda/lib/python3.6/site-packages (from thinc_gpu_ops) (1.18.2)  \n",
    "Building wheels for collected packages: thinc-gpu-ops  \n",
    "  Building wheel for thinc-gpu-ops (setup.py) ... done  \n",
    "  Created wheel for thinc-gpu-ops: filename=thinc_gpu_ops-0.0.4-cp36-cp36m-linux_x86_64.whl size=216538 sha256=d7b0f751751b16303106d6270cddb36061c5f8a248f886dca8deebb548aaf389  \n",
    "  Stored in directory: /root/.cache/pip/wheels/5b/21/70/2dabd331ebc5d25304e18c6bdbe621b16aebb1a61567733e6e  \n",
    "Successfully built thinc-gpu-ops  \n",
    "Installing collected packages: thinc-gpu-ops  \n",
    "Successfully installed thinc-gpu-ops-0.0.4  \n",
    "Collecting en_trf_bertbaseuncased_lg==2.2.0  \n",
    "  Downloading https://github.com/explosion/spacy-models/releases/download/en_trf_bertbaseuncased_lg-2.2.0/en_trf_bertbaseuncased_lg-2.2.0.tar.gz (405.8 MB)  \n",
    "     |████████████████████████████████| 405.8 MB 3.0 MB/s   \n",
    "Requirement already satisfied: spacy>=2.2.1 in /opt/conda/lib/python3.6/site-packages (from en_trf_bertbaseuncased_lg==2.2.0) (2.2.4)  \n",
    "Collecting spacy-transformers>=0.5.0  \n",
    "  Downloading spacy-transformers-0.5.1.tar.gz (59 kB)  \n",
    "     |████████████████████████████████| 59 kB 2.4 MB/s \n",
    "Requirement already satisfied: thinc==7.4.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (7.4.0)  \n",
    "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (0.4.1)   \n",
    "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (3.0.2 )  \n",
    "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (46.1.3.post20200330)  \n",
    "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (2.22.0)  \n",
    "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (0.6.0)  \n",
    "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (4.42.0)  \n",
    "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (2.0.3)  \n",
    "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (1.18.2)  \n",
    "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (1.0.2)  \n",
    "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (1.0.0)  \n",
    "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (0.9.6)  \n",
    "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (1.0.2)  \n",
    "Collecting transformers<2.1.0,>=2.0.0  \n",
    "  Downloading transformers-2.0.0-py3-none-any.whl (290 kB)  \n",
    "     |████████████████████████████████| 290 kB 7.2 MB/s     \n",
    "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (1.4.0)  \n",
    "Collecting torchcontrib<0.1.0,>=0.0.2  \n",
    "  Downloading torchcontrib-0.0.2.tar.gz (11 kB)  \n",
    "Requirement already satisfied: ftfy<6.0.0,>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (5.7)  \n",
    "Collecting dataclasses<0.7,>=0.6  \n",
    "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)  \n",
    "Requirement already satisfied: importlib_metadata>=0.20 in /opt/conda/lib/python3.6/site-packages (from spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (1.5.0)  \n",
    "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (2.8)  \n",
    "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (2019.11.28)  \n",
    "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (1.24.3)  \n",
    "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en_trf_bertbaseuncased_lg==2.2.0) (3.0.4)  \n",
    "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (2020.2.20)  \n",
    "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (0.0.38)  \n",
    "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (1.12.32)  \n",
    "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (0.1.85)  \n",
    "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy<6.0.0,>=5.0.0->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (0.1.9)  \n",
    "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib_metadata>=0.20->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (2.2.0)  \n",
    "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (0.14.1)  \n",
    "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (1.14.0)   \n",
    "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (7.1.1)  \n",
    "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0)   (0.3.3)\n",
    "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (0.9.5)  \n",
    "Requirement already satisfied: botocore<1.16.0,>=1.15.32 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0->en_trf_bertbaseuncased_lg==2.2.0) (1.15.32)  \n",
    "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0-  >en_trf_bertbaseuncased_lg==2.2.0) (0.15.2)\n",
    "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->transformers<2.1.0,>=2.0.0->spacy-transformers>=0.5.0-  >en_trf_bertbaseuncased_lg==2.2.0) (2.8.1)\n",
    "Building wheels for collected packages: en-trf-bertbaseuncased-lg, spacy-transformers, torchcontrib  \n",
    "  Building wheel for en-trf-bertbaseuncased-lg (setup.py) ... done  \n",
    "  Created wheel for en-trf-bertbaseuncased-lg: filename=en_trf_bertbaseuncased_lg-2.2.0-py3-none-any.whl size=405819943 sha256=b5c1d91dbcea394dfc3a77f5c54184e6194af2868652bcb06d2e87970a65af6d  \n",
    "  Stored in directory: /tmp/pip-ephem-wheel-cache-_iaz4fe9/wheels/ec/ee/57/ab87a942c552daca7cf69047b2180d8b71f2917b584e37d2d1  \n",
    "  Building wheel for spacy-transformers (setup.py) ... done  \n",
    "  Created wheel for spacy-transformers: filename=spacy_transformers-0.5.1-py2.py3-none-any.whl size=52835 sha256=02c3b327ee3828b6e83913e155d64057392a37382edef8b7a8af023b12b7b116  \n",
    "  Stored in directory: /tmp/pip-ephem-wheel-cache-_iaz4fe9/wheels/5b/fc/ec/3f07ac701437c5b06685db7fc64d9a963d1e6c8fcff62922f3  \n",
    "  Building wheel for torchcontrib (setup.py) ... done  \n",
    "  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-py3-none-any.whl size=7532 sha256=13b106f410c0d633a765798bc1a2aa8c9f4ab441d57c0a5634d04cada0bbc765  \n",
    "  Stored in directory: /tmp/pip-ephem-wheel-cache-_iaz4fe9/wheels/cc/fc/93/b060f81a70c264ecc80a21158efc6034712bd6144f9fe53d02  \n",
    "Successfully built en-trf-bertbaseuncased-lg spacy-transformers torchcontrib  \n",
    "Installing collected packages: transformers, torchcontrib, dataclasses, spacy-transformers, en-trf-bertbaseuncased-lg  \n",
    "  Attempting uninstall: transformers  \n",
    "    Found existing installation: transformers 2.7.0  \n",
    "    Uninstalling transformers-2.7.0:  \n",
    "      Successfully uninstalled transformers-2.7.0  \n",
    "  Attempting uninstall: dataclasses  \n",
    "    Found existing installation: dataclasses 0.7  \n",
    "    Uninstalling dataclasses-0.7:  \n",
    "      Successfully uninstalled dataclasses-0.7  \n",
    "Successfully installed dataclasses-0.6 en-trf-bertbaseuncased-lg-2.2.0 spacy-transformers-0.5.1 torchcontrib-0.0.2 transformers-2.0.0  \n",
    "✔ Download and installation successful  \n",
    "You can now load the model via spacy.load('en_trf_bertbaseuncased_lg')  \n",
    "True  \n",
    "done  \n",
    "CPU times: user 8.87 s, sys: 2.67 s, total: 11.5 s  \n",
    "Wall time: 2min 16s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and train model with custom parameters\n",
    "nlp = spacy.load(r\"/opt/conda/lib/python3.6/site-packages/en_trf_bertbaseuncased_lg/en_trf_bertbaseuncased_lg-2.2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"../working/epoch_0\")\n",
    "os.mkdir(\"../working/epoch_1\")\n",
    "os.mkdir(\"../working/epoch_2\")\n",
    "os.mkdir(\"../working/epoch_3\")\n",
    "os.mkdir(\"../working/epoch_4\")\n",
    "os.mkdir(\"../working/epoch_5\")\n",
    "os.mkdir(\"../working/epoch_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = '#$%&\\'()*+,-:;<=>@[]^_`{|}~' #removed !.?\"\\/\n",
    "punctl = [p for p in punctuation]\n",
    "emos = [\":)\", \":(\",\":-)\",\":-(\"]\n",
    "\n",
    "def preprocessHTML(raw):\n",
    "    tex = str(raw).lower()\n",
    "    cleanr = re.compile('<.*?>') # remove html tags\n",
    "    tex = re.sub(cleanr, '', tex)\n",
    "    tex = re.sub('http[s]?://\\S+', '', tex)\n",
    "    tex = re.sub(r'([)(])\\1+', r'\\1', tex).replace(\"/\\s{2,}/g\",\" \") # if it finds ) or ( repeated more than once, it just substitute with one ex )) --> )\n",
    "    tex = \" \".join([word.translate(str.maketrans({key: \"\" for key in punctl}))  if word not in emos else word for word in tex.split(\" \")]) # removes punctuation but keeps smiles\n",
    "    tex = re.sub(r'([!.?\"])\\1+', r'\\1 \\1 \\1', tex).replace(\"/\\s{2,}/g\",\" \") # substitute all repeated ! . ? and \"  with 3 instances es ??? ---> ? ? ? (and similar for . and !) #^\\w\\s\n",
    "    tex = tex.translate(str.maketrans({key: \" {0} \".format(key) for key in [\"!\",\"?\",\".\",'\"',\"/\",\"\\\\\"]})).translate(str.maketrans('','','1234567890')) # puts spaces around punctuation and removes numbers\n",
    "    tex = re.sub(r'[\\\\/]',\"\",tex)\n",
    "    tex = \" \".join(tex.strip().split())\n",
    "    tex = tex.strip()\n",
    "    return tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "docneg = []\n",
    "docpos = []\n",
    "labpos = []\n",
    "labneg = []\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename,encoding=\"utf8\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def process_docs(directory,doc,labels):\n",
    "    l = len(listdir(directory))\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=l).start()\n",
    "    i = 0\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        doc.append(preprocessHTML(load_doc(path)))\n",
    "        labels.append(filename[-5:-4])\n",
    "        i += 1\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "\n",
    "\n",
    "directoryn = r'../input/bertdata/aclImdb/train/neg'\n",
    "directoryp = r'../input/bertdata/aclImdb/train/pos'\n",
    "process_docs(directoryn,docneg,labneg)\n",
    "process_docs(directoryp,docpos,labpos)\n",
    "labpos = [int(i) for i in labpos]\n",
    "labneg = [int(i) for i in labneg]\n",
    "labposb = [1 for i in labpos]\n",
    "labnegb = [0 for i in labneg]\n",
    "print(\"len(docpos) = \", len(docpos), \"len(docneg) = \", len(docneg))\n",
    "print(\"len(labpos) = \", len(labpos), \"len(labneg) = \", len(labneg))\n",
    "if len(docpos) == len(labpos) and len(docneg) == len(labneg):\n",
    "    print(\"safe to proceed\")\n",
    "else:\n",
    "    print(\"WARNING: LABELS ARE NOT IN THE SAME NUMBER AS DATA, TERMINATION SUGGESTED\")\n",
    "\n",
    "\n",
    "train_texts = docpos[:10000] + docneg[:10000]\n",
    "train_labels = labposb[:10000] + labnegb[:10000]\n",
    "train_labels = [{'cats': {'1': label == 1,'0': label == 0}} for label in train_labels]\n",
    "print(\"len(train_labels) = \", len(train_labels), \"len(train_texts) = \", len(train_texts), len(train_texts) == len(train_labels))\n",
    "\n",
    "\n",
    "\n",
    "val_texts = docpos[10000:] + docneg[10000:]\n",
    "val_labels = labposb[10000:] + labnegb[10000:]\n",
    "print(\"len(val_labels) = \", len(val_labels), \"len(val_texts) = \", len(val_texts), len(val_texts) == len(val_labels))\n",
    "\n",
    "val_texts, val_labels = shuffle(val_texts, val_labels , random_state=0)\n",
    "\n",
    "train_data  = list(zip(train_texts, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100%|#########################################################################|  \n",
    "100%|#########################################################################|  \n",
    "len(docpos) =  12500 len(docneg) =  12500  \n",
    "len(labpos) =  12500 len(labneg) =  12500  \n",
    "safe to proceed  \n",
    "len(train_labels) =  20000 len(train_texts) =  20000 True  \n",
    "len(val_labels) =  5000 len(val_texts) =  5000 True  \n",
    "CPU times: user 48.6 s, sys: 3.01 s, total: 51.6 s  \n",
    "Wall time: 1min 25s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate roc\n",
    "def evaluate_roc(nlp):\n",
    "    docs = []\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval = len(val_texts)+1).start()  #8387 is the total number of iterations needed for an unbalanced 0.8 trainset. 0.04 of unbalanced would be approx 1540 . 4595 for lemmatized balanced. 8420 for lemmatized unbalanced\n",
    "    i = 0\n",
    "    f = 0\n",
    "    for tex in val_texts:\n",
    "        try:\n",
    "            docs.append(nlp(tex)) \n",
    "            i = i+1\n",
    "        except:\n",
    "            print(\"Error occured at val_text number: \", str(i))\n",
    "            break\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "    y_pred = [doc.cats[\"1\"] for doc in docs]\n",
    "    print(\"succeded up to: \", i)\n",
    "    roc = roc_auc_score(val_labels[:i], y_pred)\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "print(nlp.pipe_names) # [\"sentencizer\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" (bag of words) architecture.\n",
    "if 'trf_textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe(\"trf_textcat\", config={\"exclusive_classes\": True , \"architecture\": \"softmax_last_hidden\", \"words_per_batch\": 0}) \n",
    "    textcat.add_label(\"1\")\n",
    "    textcat.add_label(\"0\")\n",
    "    nlp.add_pipe(textcat, last = True)\n",
    "else:\n",
    "    print(\"something went wrong\")\n",
    "    textcat = nlp.get_pipe('trf_textcat')\n",
    "#nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "\n",
    "nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "\n",
    "\n",
    "print(\"random seeds set\")\n",
    "\n",
    "losses  = {}\n",
    "rocs = []\n",
    "run_title = \"bertbulg_80_softmax_wpb0\"\n",
    "output = \"\"\n",
    "\n",
    "print(\"strings and lists initialized\")\n",
    "\n",
    "\n",
    "#learning process\n",
    "pipe_exceptions = ['trf_textcat', \"trf_wordpiecer\",\"trf_tok2vec\"] #(\"trf_wordpiecer\", \"trf_tok2vec) are required, #https://github.com/explosion/spaCy/blob/master/examples/training/train_textcat.py\n",
    "print(\"pipe_exceptions defined\")\n",
    "print(nlp.pipe_names)\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "print(\"other_pipes defined\")\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.resume_training()\n",
    "    optimizer.alpha = 0.001\n",
    "    optimizer.trf_weight_decay = 0.005\n",
    "    optimizer.L2 = 0.0\n",
    "    learn_rate=2e-5\n",
    "    batch_size  = 8\n",
    "    learn_rates = cyclic_triangular_rate(learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size)\n",
    "    for epoch in range(7):\n",
    "        random.shuffle(train_data)\n",
    "        print(\"data shuffled\")\n",
    "        # Create the batch generator with batch size = 8\n",
    "        batches = minibatch(train_data, size=batch_size)\n",
    "        print(\"batches created\")\n",
    "        optimizer.trf_lr = next(learn_rates)\n",
    "        # Iterate through minibatches\n",
    "        pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=2525).start()  \n",
    "        i = 0                                                                    \n",
    "        for batch in batches:\n",
    "            # Each batch is a list of (text, label) but we need to\n",
    "            # send separate lists for texts and labels to update().\n",
    "            # This is a quick way to split a list of tuples into lists\n",
    "            texts1, labels = zip(*batch)\n",
    "            nlp.update(texts1, labels, sgd = optimizer, losses=losses, drop = 0.1)\n",
    "            i += 1\n",
    "            pbar.update(i)\n",
    "        pbar.finish()\n",
    "        print(\"i = \",  i)\n",
    "        with nlp.use_params(optimizer.averages):   \n",
    "            rocs.append(evaluate_roc(nlp))\n",
    "            nlp.to_disk(r\"../working/\"+\"epoch_\"+str(epoch)+\"/bertbulg_80_softmax_wpb0_\"+str(epoch))\n",
    "            output += f\"    epoch = {epoch}, roc = {rocs[-1]} , loss = {losses} \\n \"  \n",
    "            print( \"epoch = \",epoch, \"roc = \" , rocs[-1] ,\"loss =\",losses  ,\"i = \", i)  \n",
    "\n",
    "\n",
    "\n",
    "with open(r\"../working/\" + run_title + \".txt\", \"a\") as file: #name\n",
    "    file.write(run_title +\"\\n\" + output)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['sentencizer', 'trf_wordpiecer', 'trf_tok2vec'] <br>\n",
    "pheww<br>\n",
    "  0%|                                                                         |<br>\n",
    "random seeds set<br>\n",
    "strings and lists initialized<br>\n",
    "pipe_exceptions defined<br>\n",
    "['sentencizer', 'trf_wordpiecer', 'trf_tok2vec', 'trf_textcat']<br>\n",
    "other_pipes defined<br>\n",
    "data shuffled<br>\n",
    "batches created<br>\n",
    "100%|#########################################################################|<br>\n",
    "  0%|                                                                         |<br>\n",
    "i =  2500<br>\n",
    "evaluate1<br>\n",
    "100%|#########################################################################|<br>\n",
    "Error occured at val_text number:  3496<br>\n",
    "evaluate2<br>\n",
    "succeded up to:  3496<br>\n",
    "evaluate3<br>\n",
    "evaluate4<br>\n",
    "<br>\n",
    "  0%|                                                                         |<br>\n",
    "epoch =  0 roc =  0.97479066403316 loss = {'trf_textcat': 6.877855688240999} i =  2500<br>\n",
    "data shuffled<br>\n",
    "batches created<br>\n",
    "100%|#########################################################################|<br>\n",
    "  0%|                                                                         |<br>\n",
    "i =  2500  \n",
    "evaluate1  \n",
    "100%|#########################################################################|   \n",
    "Error occured at val_text number:  3795 \n",
    "evaluate2  \n",
    "succeded up to:  3795  \n",
    "evaluate3  \n",
    "evaluate4  \n",
    "  \n",
    "  0%|                                                                         |  \n",
    "epoch =  1 roc =  0.9755197732706444 loss = {'trf_textcat': 10.744955211586598} i =  2500  \n",
    "data shuffled  \n",
    "batches created  \n",
    "100%|#########################################################################|  \n",
    "  0%|                                                                         |  \n",
    "i =  2500  \n",
    "evaluate1  \n",
    "100%|#########################################################################|  \n",
    "Error occured at val_text number:  3878  \n",
    "evaluate2  \n",
    "succeded up to:  3878  \n",
    "evaluate3  \n",
    "evaluate4  \n",
    "  \n",
    "  0%|                                                                         |  \n",
    "epoch =  2 roc =  0.9751145844637511 loss = {'trf_textcat': 12.884738852677117} i =  2500  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
